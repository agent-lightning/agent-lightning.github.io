<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Agent Lightning ⚡ Blog</title>
    <link>https://agent-lightning.github.io/</link>
    <description>Recent content on Agent Lightning ⚡ Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Nov 2025 00:00:00 +0800</lastBuildDate>
    <atom:link href="https://agent-lightning.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tinker X Agent Lightning</title>
      <link>https://agent-lightning.github.io/posts/tinker/</link>
      <pubDate>Wed, 19 Nov 2025 00:00:00 +0800</pubDate>
      <guid>https://agent-lightning.github.io/posts/tinker/</guid>
      <description>&lt;h1 id=&#34;tuning-any-ai-agent-with-tinker-x-agent-lightning&#34;&gt;Tuning ANY AI agent with Tinker X Agent-lightning&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;Yuge Zhang&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Nov. 2025&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tinker is the first product built by an all-star company called &lt;strong&gt;Thinking Machine Lab&lt;/strong&gt;, whose team members come from leading organizations such as OpenAI. Notable members include former OpenAI CTO &lt;strong&gt;Mira Murati&lt;/strong&gt;; &lt;strong&gt;John Schulman&lt;/strong&gt;, the first author of PPO; &lt;strong&gt;Barret Zoph&lt;/strong&gt;, a leading scientist in AutoML (the area I previously worked in); and well-known Asian researchers like &lt;strong&gt;Danqi Chen&lt;/strong&gt; and &lt;strong&gt;Lilian Weng&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>No More Retokenization Drift</title>
      <link>https://agent-lightning.github.io/posts/no-more-retokenization-drift/</link>
      <pubDate>Tue, 18 Nov 2025 01:13:13 +0800</pubDate>
      <guid>https://agent-lightning.github.io/posts/no-more-retokenization-drift/</guid>
      <description>&lt;h1 id=&#34;no-more-retokenization-drift-returning-token-ids-via-openai-compatible-apis-matters-in-agent-rl&#34;&gt;No More Retokenization Drift: Returning Token IDs via OpenAI Compatible APIs Matters in Agent RL&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;Agent Lightning (AGL) team&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Date: Oct. 2025&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR.&lt;/strong&gt; Agent often calls LLMs via OpenAI‑compatible endpoints, which previously return only string-based inputs and outputs. In &lt;strong&gt;agent RL&lt;/strong&gt;, this can lead to inconsistencies between training and inference due to the phenomenon we call &lt;strong&gt;Retokenization Drift&lt;/strong&gt;. This phenomenon occurs because tokens are detokenized during inference and subsequently retokenized during training; the two sets of tokens may differ even though their corresponding strings are identical. Now, you can ask vLLM’s OpenAI‑compatible endpoints to return the &lt;strong&gt;exact token IDs&lt;/strong&gt; for both prompts and generated responses. Pass &lt;code&gt;&amp;quot;return_token_ids&amp;quot;: true&lt;/code&gt; to &lt;code&gt;/v1/chat/completions&lt;/code&gt; or &lt;code&gt;/v1/completions&lt;/code&gt; and you’ll receive &lt;code&gt;prompt_token_ids&lt;/code&gt; and &lt;code&gt;token_ids&lt;/code&gt; alongside the regular text output. This makes &lt;strong&gt;agent RL&lt;/strong&gt; robust, as no more drift will happen. This pairs perfectly with Agent Lightning, where each model call is viewed as separate update sample without stitching; just log the returned IDs via &lt;code&gt;return_token_ids&lt;/code&gt; enabled.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hello World!</title>
      <link>https://agent-lightning.github.io/posts/hello-world/</link>
      <pubDate>Sat, 15 Jun 2024 12:00:00 +0000</pubDate>
      <guid>https://agent-lightning.github.io/posts/hello-world/</guid>
      <description>&lt;p&gt;Hello World!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
